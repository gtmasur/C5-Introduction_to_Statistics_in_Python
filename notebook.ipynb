{"cells":[{"source":"# Introduction to Statistics in Python\n\n1. Summary Statistics\n    - What is statistics?\n    - Measures of center\n    - Measures of spread\n2. Random Numbers and Probability\n    - What are the chances?\n    - Discrete distributions\n    - Continuous distributions\n    - The binomial distribution\n3. More Distributions and the Central Limit Theorem\n    - The normal distribution\n    - The central limit theorem\n    - The Poisson distribution\n    - More probability distributions\n4. Correlation and Experimental Design\n    - Correlation\n    - Correlation caveats\n    - Design of experiments","metadata":{},"id":"20c73b4c-626a-4674-b2ca-0f4c47f60628","cell_type":"markdown"},{"source":"# Importing numpy and pandas\nimport numpy as np\nimport pandas as pd\n\n# Importing the course datasets\ndeals = pd.read_csv(\"datasets/amir_deals.csv\")\nhappiness = pd.read_csv(\"datasets/world_happiness.csv\")\nfood = pd.read_csv(\"datasets/food_consumption.csv\")","metadata":{"executionTime":33,"lastSuccessfullyExecutedCode":"# Importing the course packages\nimport numpy as np\nimport pandas as pd\n\n# Importing the course datasets\ndeals = pd.read_csv(\"datasets/amir_deals.csv\")\nhappiness = pd.read_csv(\"datasets/world_happiness.csv\")\nfood = pd.read_csv(\"datasets/food_consumption.csv\")","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"cc8f4fbc-8936-468a-b789-fe76aae1fc03","cell_type":"code","execution_count":2,"outputs":[]},{"source":"# 1. Summary of statistics\n\n## What is statistics?\n- the field of statistics: the practice and study of collecting and analyzing data.\n    - Descriptive statistics: describe and summarize data\n    - Inferential statistics: use a sample data to make inferences about a larger population.\n- Data:\n    1. Numeric (quantitative) data: \n        - continuous (measure) \n        - discrete (counter)\n    2. Categorial (qualitative) data:\n        - nominal (unordered): no inherent order\n        -  ordinal (ordered)\n- Importance of data type: Data type dictates what kinds of summary statistics and visualizations make sense for your data.\n\n## Measures of center\n- meaning of mean, median and mode:\n    - mean: the sum of all the data points divided by the total number of data points.\n    - median: the middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median\n    - mode: the most frequent value in data, often used for categorical variables.\n- codes to run:\n    - df['col_to_summarize'].agg([np.mean, np.median])\n    - import statistics,  statistics.mode(df['col_to_find_mode'])\n- Histogram: the distribution of observations divided into bins.\n- which measure to use:\n    - symmetrical data:\n        - the mean is more sensitive to extreme values, works better with symettrical data.\n        - the mean and median are so close to each other.\n    - skewed data (not symmetrical):\n        - median is better to use\n        - left-skewed data has a long tail of low values, mean < median.\n        - right-skewed data has a long tail of high values, mean > median.\n        - the mean is pulled in the direction of the skew.\n\n## Measures of spread\n- spread: describes how spread apart or close together the data points are.\n- **variance:** average distance from each data point to the data's mean\n    - Calculating variance: \n        1. subtract mean from each data point\n        2. square each distance\n        3. sum squared distances\n        4. divide by the number of data points-1\n    - np.var(df['col'], ddof=1), ddof must be set to 0 for the above formula.\n    - the higher the variance, the more spread out the data is.\n- **standard deviation:** \n    - calculated by taking the square root of the variance.\n    - np.std(df['col'], ddof=1)\n- **mean absolute deviation:** takes the absolute value of the distances to the mean, and then takes the mean of those differences\n- quantiles (percentiles): \n    - np.quantile(df[...], percentile)\n    - splits up the data into some number of equal parts.\n    - boxplot uses quartiles, plt.boxplot(...)\n    - possible to us elinspace for percentile. np.linspace(start,stop, num_of_interval)\n- **interquartile range**: \n    - the distance between the 25th and 75th percentile\n    - the height of the box in boxplot\n    - use quantile function: np.quantile(...,0.75) - np.quantile(....,0.25)\n    - or from scipy.stats import iqr\n- **outliears:**    \n    - data points that are substantially different from the others.\n    - a data point is an outlier if \n        - lower threshold: data < Q1 - 1.5xIQR\n        - upper threshold: data > Q3 + 1.5xIQR\n- df.describe(): summarizes the statistics","metadata":{},"cell_type":"markdown","id":"1742b1ca-494a-4023-9b9b-3b93b892b8b2"},{"source":"### Measures of center ###\n\n#-----------------------------------\n# Mean and median\n\n# Working with the food_consumption dataset from 2018 Food Carbon Footprint Index.\n# The food_consumption dataset contains the number of kgs of food consumed per person\n# per year in each country and food category (consumption), and its carbon footprint \n# (co2_emissions) measured in kilograms of carbon dioxide, or CO2.\n\nprint(food_consumption.head(), food_consumption.shape)\n\n# Import numpy with alias np\nimport numpy as np\n\n# Subset country for USA: usa_consumption\nusa_consumption = food_consumption.query('country==\"USA\"')\n\n# Calculate mean consumption in USA\nprint(usa_consumption['consumption'].mean())\n\n# Calculate median consumption in USA\nprint(usa_consumption['consumption'].median())\n\n#--------------------------------------------\n# Mean vs. median\n\n# Import matplotlib.pyplot with alias plt\nimport matplotlib.pyplot as plt\n\n# Subset for food_category equals rice: rice_consumption\nrice_consumption = food_consumption[food_consumption['food_category'] == 'rice']\n# rice_consumption = food_consumption.query('food_category == \"rice\"']\n\n# Histogram of co2_emission for rice and show plot\nrice_consumption['co2_emission'].hist()\nplt.show()\n# the plot is right-skewed shape\n\n# Calculate mean and median of co2_emission with .agg()\nprint(rice_consumption['co2_emission'].agg([np.mean, np.median]))\n","metadata":{"executionTime":16,"lastSuccessfullyExecutedCode":"# Add your code snippets here"},"cell_type":"code","id":"fb5e8360-ad35-430d-a8d1-559df0c5baae","outputs":[],"execution_count":3},{"source":"### Measures of spread ###\n\n#-------------------------------------------------\n# Variance and standard deviation\n\nprint(food_consumption)\n\n# Print variance and sd of co2_emission for each food_category\nprint(food_consumption.groupby('food_category')['co2_emission'].agg([np.var,np.std]))\n\nplt.clf()\n# Create histogram of co2_emission for co2_emission 'beef'\nfood_consumption[food_consumption['food_category']=='beef']['co2_emission'].hist()\nplt.show()\n\nplt.clf()\n# Create histogram of co2_emission for food_category 'eggs'\nfood_consumption[food_consumption['food_category']=='eggs']['co2_emission'].hist()\nplt.show()\n\n#-------------------------------------------------\n# Quartiles, quantiles, and quintiles\n\n\nprint(food_consumption)\nprint(np.linspace(0,1,5))\n\n# Calculate the quartiles of co2_emission\nprint(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,5)))\n\n# Calculate the quintiles ( into 5 pieces) of co2_emission\nprint(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,6)))\n\n# Calculate the deciles ( into 10 pieces) of co2_emission\nprint(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,11)))\n\n#-------------------------------------------------\n# Finding outliers using IQR\n\n# TASK: find outliers in food_consumption dataset.\n\n# IQR is another way of measuring spread that's less influenced \n# by outliers, and also often used to find outliers.\n\n# Calculate total co2_emission per country: emissions_by_country\nemissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()\nprint(emissions_by_country)\n\n# Compute the first and third quantiles and IQR of emissions_by_country\nq1 = np.quantile(emissions_by_country, 0.25)\nq3 = np.quantile(emissions_by_country, 0.75)\niqr = q3 - q1\nprint(q1,q3, iqr)\n\n# Calculate the lower and upper cutoffs for outliers\nlower = q1 - 1.5 * iqr\nupper = q3 + 1.5 * iqr\nprint(lower, upper)\n#print(emissions_by_country[emissions_by_country<400])\n\n# Subset emissions_by_country to find outliers\noutliers = emissions_by_country[(emissions_by_country < lower) \\\n            | (emissions_by_country > upper)]\nprint(outliers)\n\n# COMMENT:  Argentina has a substantially higher amount of CO2 emissions \n# per person than other countries in the world.\n\n","metadata":{},"cell_type":"code","id":"fd0e7fe2-d24e-4c24-838a-8f03b1b62a67","outputs":[],"execution_count":null},{"source":"# 2. Random Numbers and Probability\n\n## What are the chances?\n- Measuring chance: probability\n- Sampling from a DataFrame: df.sample()\n    - seeding a random number: a number that Python's random number generator uses as a starting point\n    - np.random.seed(), df.sample(): gives the same result each time\n    - sampling twice without replacement: df.sample(2) (not choosing the same person twice)\n    - sampling twice with replacement: df.sample(5, replace=True)\n- **independent events**: \n    - Two events are independent if the probability of the second event isn't affected by the outcome of the first event. \n    - sampling with replacement = each pick is independent\n- **dependent events**: sampling without replacement = each pcik is dependent\n\n## Discrete distributions\n- A **probability distribution** describes the probability of each possible outcome in a scenario. \n- Discrete distributions describes probabilities of discerete outcomes.\n- Rolling a fair dice: discerete uniform distribution\n- **expected value**: \n    - mean of a probability distribution.\n    - calculated by multiplying each value by its probability and summing.\n- use bar plot to visualize a distribution\n- **probability = area**:\n    - $P(X ≤ x)$ represents the probability that a random variable $X$ with a uniform distribution takes a value less than or equal to $x$.\n    - probabilities of different outcomes are calculated by taking areas of the probability distribution\n    - $(P(die \\ roll) \\leq 2) = P(die\\ roll=1)+ P(die\\ roll=2)$\n- Law of large numbers: as the size of your sample increases, the sample mean will approach the theoretical mean.\n\n## Continuous distributions\n- To understand, we work on a specific example: The city bus arrives once every twelve minutes, so if you show up at a random time, you could wait anywhere \n    - from 0 minutes if you just arrive as the bus pulls in, \n    - up to 12 minutes if you arrive just as the bus leaves.\n- **Continunous uniform distribution:**\n    - Instead of indivual blocks, a line plot describes the prob. here.\n    - The same probability of waiting any time from 0 to 12 minutes. This is called the continuous uniform distribution.\n- **probability = area**:\n    -  $(P(4 \\leq \\text{wait time} \\leq 7)? $\n        - areawise computation : $(7-4)*1/12 =3/12$\n    - **uniform distribution in Python**:\n        - calculate $P(\\text{wait time} \\leq 7)$\n            - from scipy.stats import uniform, uniform.cdf(7,0,12)\n            - uniform.cdf(value x, lower limit, upper limit)\n        - $P(\\text{wait time} \\geq 7) = 1 - P(\\text{wait time} \\leq 7)$\n            - = 1- uniform.cdf(value x, lower limit, upper limit)\n        - $P(4 \\leq \\text{wait time} \\leq 7)= P(\\text{wait time} \\leq 7) - P(\\text{wait time} \\leq 4) $\n- generating random numbers ccording to the uniform distribution:\n    - uniform.rvs(min, max, # of random values)\n    - uniform.rvs(0,5,10) = 10 random values are generated between 0 and 5.\n- Other continuous dist. have different shape. No matter the shape of the distribution, the area beneath it must always equal 1.\n- NOTE: Learned about the two different variants of the uniform distribution: the discrete uniform distribution, and the continuous uniform distribution.\n\n## The binomial distribution (discrete)\n- A distribution with binary outcomes as a 1 and a 0, a success or a failure, and a win or a loss.\n    - from scipy.stats import binom\n    - binom.rvs(# of coins, prob. of heads/success, size = # of trials)\n    - $binom.rvs(1,0.5, size=1)$:  flip 1 coin, with a 50% probability of heads, 1 time.\n- one flip many times: \n    - $binom.rvs(1,0.5, size=8)$ for 8 coin flips\n    - returns a set of ones including ones and zeros\n- many flips one time:  \n    - $binom.rvs(8,0.5, size=1)$ flip 8 coins 1 time\n    - returns total number of heads/success\n- many flips many times:\n    - $binom.rvs(3, 0.5, size=10)$ flip 3 coins 10 times\n    - returns 10 numbers, each representing the total number of heads from each set of flips.\n- other probabilities: possible, binom.rvs(3, 0.25, size=10)\n- **binomial dist.**:\n    - the probability of the number of successes in a sequence of independent trials. \n    - In other words, the probability of getting some number of heads in a sequence of coin flips. \n    - $n$: total number of trials, $p$: probability of success\n    - indeed, binom.rvs(# of coins, p, size=n)\n- calculating probability:\n    - getting 7 heads out of 10 coins:\n        - $P(heads=7)$ = binom.pmf(7, 10, 0.5)\n        - binom.pmf(# heads, # trials, prob. of heads)\n    - getting a number of successes less than or equal to 7:\n        - $P(heads \\leq 7)$ = binom.cdf(7, 10, 0.5)\n- **expected value**: $n \\times p$ \n- In binomial dist, each trial must be **independent**.","metadata":{},"cell_type":"markdown","id":"94d86b01-60d3-481d-9cd2-1ddefb3e1132"},{"source":"###  What are the chances? ###\n\n#-------------------------------------------------\n# Calculating probabilities\n\n# Randomly select a few of the deals that Amir has worked on over the past year.\n# Before you start selecting deals, you'll first figure out what \n# the chances are of selecting certain deals.\n\nprint(amir_deals)\n\n# Count the deals for each product\ncounts = amir_deals['product'].value_counts()\nprint(counts)\n\n# Calculate probability of picking a deal with each product\nprobs = counts/counts.sum()\nprint(probs)\n\n#-------------------------------------------------\n# Sampling deals\n\n#In the previous exercise, you counted the deals Amir worked on. \n# Now, randomly pick five deals, reach out to each customer and \n# ask if they were satisfied with the service they received. \n# Do it both with and without replacement.\n\n# Set random seed\nnp.random.seed(24)\n\n# Sample 5 deals without replacement\nsample_without_replacement = amir_deals.sample(5)\nprint(sample_without_replacement)\n\n# Sample 5 deals with replacement\nsample_with_replacement = amir_deals.sample(5, replace=True)\nprint(sample_with_replacement)\n\n# Comment: without replacement must be used in this case.\n","metadata":{},"cell_type":"code","id":"93ad1d06-9a06-4467-ad99-62d255f495e7","outputs":[],"execution_count":null},{"source":"### Discrete distributions ###\n\n#-------------------------------------------------\n# Creating a probability distribution\n\n# In a restaurant, optimize its seating space based on the size of the groups that come most often. \n# On one night, there are 10 groups of people waiting to be seated at the restaurant, \n# Instead of being called in the order they arrived, they will be called randomly. \n\n# TASK: Investigate the probability of groups of different sizes getting picked first.\n\nprint(restaurant_groups)\n\n# Create a histogram of restaurant_groups etting bins to [2, 3, 4, 5, 6] \nrestaurant_groups['group_size'].hist(bins=[2,3,4,5,6])\nplt.show()\n\n# ---- ************** ----------\n# Create probability distribution \n# to calculate the prob. of randomly selecting a group of each size\n# prob dist:  the prob. of each possible outcome in a scenario.\nsize_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n\n# Reset index and rename columns\nsize_dist = size_dist.reset_index()\nsize_dist.columns = ['group_size', 'prob']\nprint(size_dist)\n\n# Calculate expected value\nexpected_value = np.sum(size_dist['group_size']*size_dist['prob'])\nprint(expected_value)\n\n# Subset groups of size 4 or more\ngroups_4_or_more = size_dist.query('group_size>=4')\nprint(groups_4_or_more)\n\n# Sum the probabilities of groups_4_or_more\n# P(a>=4), a: the number of a group of people that are randomly selected\nprob_4_or_more = groups_4_or_more['prob'].sum()\nprint(prob_4_or_more)\n\n# NOTE: calculations of prob. dist and expected value is really important!","metadata":{},"cell_type":"code","id":"b087d985-86bf-4fa5-8dff-e7711d9d8ba9","outputs":[],"execution_count":null},{"source":"### Continuous distributions ###\n\n#-------------------------------------------------\n# Data back-ups\n\n# The sales software automatically back itself up exactly every 30 minutes. \n# How long does one have to wait for the back-up?\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5,0,30)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - prob_less_than_5\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20,0,30) - uniform.cdf(10,0,30)\nprint(prob_between_10_and_20)\n\n#-------------------------------------------------\n# Simulating wait times\n\n# To give a better idea of how long one has to wait, \n# simulate 1000 waiting times and create a histogram.\n\n# Set random seed to 334\nnp.random.seed(334)\n\n# Import uniform\nfrom scipy.stats import uniform\n\n# Generate 1000 wait times between 0 and 30 mins\nwait_times = uniform.rvs(0, 30, size=1000)\n\n# Create a histogram of simulated times and show plot\nplt.hist(wait_times)\nplt.show()\n","metadata":{"executionCancelledAt":null,"executionTime":4695,"lastExecutedAt":1728642806048,"lastExecutedByKernel":"8a80f81f-c6cd-4ae0-9cda-f69d79b341ab","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"### Continuous distributions ###\n\n#-------------------------------------------------\n# Data back-ups\n\n# The sales software automatically back itself up exactly every 30 minutes. \n# How long does one have to wait for the back-up?\n\n# Min and max wait times for back-up that happens every 30 min\nmin_time = 0\nmax_time = 30\n\n# Import uniform from scipy.stats\nfrom scipy.stats import uniform\n\n# Calculate probability of waiting less than 5 mins\nprob_less_than_5 = uniform.cdf(5,0,30)\nprint(prob_less_than_5)\n\n# Calculate probability of waiting more than 5 mins\nprob_greater_than_5 = 1 - prob_less_than_5\nprint(prob_greater_than_5)\n\n# Calculate probability of waiting 10-20 mins\nprob_between_10_and_20 = uniform.cdf(20,0,30) - uniform.cdf(10,0,30)\nprint(prob_between_10_and_20)\n\n\n\n\n#-------------------------------------------------\n\n\n#-------------------------------------------------","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"84738990-2360-4a84-b492-d01966c1af82","outputs":[{"output_type":"stream","name":"stdout","text":"0.16666666666666666\n0.8333333333333334\n0.3333333333333333\n"}],"execution_count":1},{"source":"### The binomial distribution ###\n\n#-------------------------------------------------\n# Simulating sales deals\n\n# Assume that Amir usually works on 3 deals per week, and overall, \n# he wins 30% of deals he works on. Each deal has a binary outcome: \n# it's either lost, or won, so you can model his sales deals with a \n# binomial distribution. \n# TASK: Simulate a year's worth of his deals so he can better understand his performance.\n\n# Import binom from scipy.stats\nfrom scipy.stats import binom\n\n# Set random seed to 10\nnp.random.seed(10)\n\n# Simulate a single deal\nprint(binom.rvs(1, 0.3, size=1))\n\n# Simulate 1 week of 3 deals\nprint(binom.rvs(3, 0.3, size=1))\n\n# Simulate 52 weeks of 3 deals\ndeals = binom.rvs(3, 0.3, size=52)\n\n# Print mean deals won per week\nprint(deals.sum()/52)\n\n#-------------------------------------------------\n# Calculating binomial probabilities\n\n# Assume that Amir wins 30% of deals.\n# Calculate how likely he is to close a certain number of deals each week. \n\n# Probability of closing 3 out of 3 deals\nprob_3 = binom.pmf(3,3,0.3)\nprint(prob_3)\n\n# Probability of closing <= 1 deal out of 3 deals\n# binom.pmf(# heads, # trials, prob. of heads)\nprob_less_than_or_equal_1 = binom.cdf(1,3,0.3)\nprint(prob_less_than_or_equal_1)\n\n# Probability of closing > 1 deal out of 3 deals\nprob_greater_than_1 = 1 - binom.cdf(1,3,0.3)\nprint(prob_greater_than_1)\n\n#-------------------------------------------------\n# How many sales will be won?\n\n# How many deals does Amir can expect to close each week with different win rate?\n# Expected value = n x p\n\n# Calculate the expected number of sales out of the 3 he works on \n# that Amir will win each week if he maintains his 30% win rate.\n# Expected number won with 30% win rate\nwon_30pct = 3 * 0.3\nprint(won_30pct)\n\n# Expected number won with 25% win rate\nwon_25pct = 3 * 0.25\nprint(won_25pct)\n\n# Expected number won with 35% win rate\nwon_35pct = 3 * 0.35\nprint(won_35pct)","metadata":{},"cell_type":"code","id":"4ddfbddd-20f5-43d4-adff-408cb1b30adc","outputs":[],"execution_count":null},{"source":"# 3. More Distributions and the Central Limit Theorem\n\n## The normal distribution (continuous)\n- An important distr. since many statistical methods rely on it and it applies to real-world situations.\n- Features:\n    - symmetrical\n    - area under the cure = 1 (as any other distr.)\n    - the distr. never takes the value of $0$ even if at the tail ends.\n- Special case: mean=$0$, std=$1$ (standard normal distr)\n- Areas under the normal distribution: $68-95-99.7$ rule (the area of $68$ falls under 1 std, and so on...)\n- Approximating data with normal distribution:\n    - from scipy.stats import norm, norm.cdf(x, mean, std)\n    - reverse: 1 - norm.cdf(x, mean, std)\n- Percentailes from the distr: \n    - norm.ppf(0.9, mean, std): what height $90\\%$ of women are shorter than?\n    - norm.ppf((1-0.9), mean, std): what height $90\\%$ of women are taller than?\n- Generatin grandom numbers: \n    - norm.rvs(mean, std, size=10) \n    - generate 10 random numbers passing in the dist's mean and std\n\n## The central limit theorem (CLT)\n- sampling distribution: \n    - sampling distribution of the sample mean.\n    - for example: rolling the dice 5 times and taking the mean of the outcome. Repeating this 10 times gives the sampling distribution.\n- **The central limit theorem**: A sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.\n    - The samples are taken randomly and are independent, then the theorem applies.\n    - CLT applies to other summary statistics too:\n        - standard deviation\n        - proportions (sampling distribution of proportions, for example: the proportion that Ali will appear in the sample case)\n- mean of sampling distribution: \n    - when sampling distributions are normal, their mean can be found as an estimate of a distribution's mean.\n- CLT is useful for:\n    - estimating characteristics of an underlying distribution when the underlying distr. is not known.\n    - more easily estimate characteristics of large populations. (if one time and resources to collecte data on everyone, one can collect several smaller samples and create a sampling distribution to estimate what the mean or standard deviation is.)\n- **From exercise:** Regardless of the shape of the distribution you're taking sample means from, the central limit theorem will apply if the sampling distribution contains enough sample means.\n\n## The Poisson distribution (discrete)\n- Poisson process: a process where events appear to happen at a certain rate, but completely at random. Examples:\n    - the number of animals adopted from an animal shelter each week, \n    - the number of people arriving at a restaurant each hour, or \n    - the number of earthquakes per year in California. \n- Poisson distribution: the probability of some number of events happening over a fixed period of time.\n- The time unit doesn't matter as long as it's consistent. \n- $\\lambda$: describes Poisson distr., \n    - represents average number of events per time interval\n    - the distr. has its peak at $\\lambda$ value.\n    - the expected value is $\\lambda$.\n- probability of a single value:\n    - from scipy.stats import poisson, poisson.pmf(x, $\\lambda$)\n- probability of less than or equal to a value: poisson.cdf(x, $\\lambda$)\n- sampling from Poisson distr: poisson.rvs($\\lambda$, size=10)\n- The CLT still applies!\n\n## More probability distributions\n\n### Exponential distribution (continous)\n- The distr. represents the probability of a certain time passing between Poisson events. \n- For example,\n    - the probability of more than 1 day between adoptions, \n    - the probability of fewer than 10 minutes between restaurant arrivals, \n    - the probability of 6-8 months passing between earthquakes. \n- Just like the Poisson distribution, \n- Also uses the same lambda value, which represents the rate. Note that lambda and rate mean the same value in this context.\n- Example: one customer service ticket is created every 2 minutes.\n    - = $\\lambda=0.5$ customer service tickets created each minute\n    - The rate $\\lambda$ affects the shape of the distribution and how steeply it declines.\n- **expected value**: \n    - in terms of rate (Poisson):  $\\lambda=0.5$ requests per min  (frequencs or # of events in a certain time)\n    - in terms of between events (exponential) : $1/\\lambda=1/0.5=2$, 1 request per 2 mins\n- probability calculation: expon.cdf(x, scale=$1/\\lambda$), scale=2\n\n### Students's (T) distribution\n- The distr. has similar to the normal distribution, but not quite the same.\n-  the tail of the t-distribution with one degree of freedom are thicker, which means that, observations are more likely to fall further from the mean: higher variance than the normal distribution.\n- degrees of freedom (df): a parameter which affects the thickness of the distribution's tails.\n    - Lower df: thicker tails and a higher standard deviation. \n    - Higher df: closer to normal distribution.\n \n### Log-normal distribution\n- In log-normla distr., variables have a logarithm that is normally distributed. \n- The distribution is skewed, unlike the normal distribution. \n- Examples: \n    - the length of chess games, \n    - blood pressure in adults, and \n    - the number of hospitalizations in the 2003 SARS outbreak.","metadata":{},"cell_type":"markdown","id":"7efe62d1-8c8f-4eb3-bcc9-972d19246124"},{"source":"### The normal distribution ###\n\n#-------------------------------------------------\n# Distribution of Amir's sales\n\n# As each deal Amir worked on (both won and lost) was different, \n# each was worth a different amount of money. As part of Amir's performance review, \n# we estimate the probability of him selling different amounts.\n# Before, first, we determine what kind of distribution the amount variable follows.\n\nprint(amir_deals)\n\n# Histogram of amount with 10 bins and show plot\namir_deals['amount'].hist(bins=10)\nplt.show()\n\n#-------------------------------------------------\n# Probabilities from the normal distribution\n\n# The amount column of amir_deals follow a normal distribution with a mean of \n# 5000 dollars and a standard deviation of 2000 dollars. \n\n# Probability of deal < 7500\nprob_less_7500 = norm.cdf(7500, 5000, 2000)\nprint(prob_less_7500)\n\n# Probability of deal > 1000\nprob_over_1000 = 1 - norm.cdf(1000, 5000, 2000)\nprint(prob_over_1000)\n\n# Probability of deal between 3000 and 7000\nprob_3000_to_7000 = norm.cdf(7000, 5000, 2000) - norm.cdf(3000, 5000, 2000)\nprint(prob_3000_to_7000)\n\n# Calculate amount that 25% of deals will be less than\npct_25 = norm.ppf(0.25, 5000, 2000)\nprint(pct_25)\n\n#-------------------------------------------------\n# Simulating sales under new market conditions\n\n# The company's financial analyst is predicting that next quarter, \n# the worth of each sale will increase by 20% and the std of each sale's\n# worth will increase by 30%. Simulate new sales amounts using the normal distribution \n\n# Calculate new average amount\nnew_mean = 5000*1.2\n\n# Calculate new standard deviation\nnew_sd = 2000*1.3\n\n# Simulate 36 new sales\nnew_sales = norm.rvs(new_mean, new_sd, size=36)\n\n# Create histogram and show\nplt.clf()\nplt.hist(new_sales, bins=20)\nplt.show()\n","metadata":{},"cell_type":"code","id":"552e961d-fba7-4dd2-b0ed-b3ed83151205","outputs":[],"execution_count":null},{"source":"### The central limit theorem ###\n\n#-------------------------------------------------\n# The CLT in action\n\n# Examine the num_users column of amir_deals more closely, which contains \n# the number of people who intend to use the product Amir is selling.\n# Do this by creating sampling distribution.\n\n# Set seed to 104\nnp.random.seed(104)\nprint(amir_deals)\n\n# i) Create a histogram of num_users and show\namir_deals['num_users'].hist()\nplt.show()\n\n# ii) Sample 20 num_users with replacement from amir_deals\nsamp_20 = amir_deals['num_users'].sample(20, replace=True)\n# Take mean of samp_20\nprint(samp_20.mean())\n\n# iii) Repeat this 100 times using a for loop \nsample_means = []\nfor i in range(100):\n  # Take sample of 20 num_users\n  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n  # Calculate mean of samp_20\n  samp_20_mean = np.mean(samp_20)\n  # Append samp_20_mean to sample_means\n  sample_means.append(samp_20_mean)\n  \n# Convert to Series and plot histogram\nsample_means_series = pd.Series(sample_means)\nsample_means_series.hist()\nplt.show()\n\n#-------------------------------------------------\n# The mean of means\n\n# By calculation the average number of users (num_users) is per deal, \n# see if Amir's deals have more or fewer users than the company's average deal.\n# Over the past year, the company has worked on more than ten thousand deals, \n# instead of compiling all the data, estimate the mean by taking several random samples of deals.\n\n# Set seed to 321\nnp.random.seed(321)\n\nsample_means = []\n# Loop 30 times to take 30 means\nfor i in range(30):\n  # Take sample of size 20 from num_users col of all_deals with replacement\n  cur_sample = all_deals['num_users'].sample(20, replace=True)\n  # Take mean of cur_sample\n  cur_mean = cur_sample.mean()\n  # Append cur_mean to sample_means\n  sample_means.append(cur_mean)\n\n# Print mean of sample_means\nprint(pd.Series(sample_means).mean())\n\n# Print mean of num_users in amir_deals\nprint(amir_deals['num_users'].mean())\n\n# Comment: Amir's average number of users is very close to the overall average, \n# so it looks like he's meeting expectations.\n","metadata":{},"cell_type":"code","id":"1b6c91d8-ac0e-4314-980d-47db60e034d7","outputs":[],"execution_count":null},{"source":"### The Poisson distribution ###\n\n#-------------------------------------------------\n# Tracking lead responses\n\n# The company uses sales software to keep track of new sales leads. \n# It organizes them into a queue so that anyone can follow up on one in free time. \n# Since the number of lead responses is a countable outcome over a period of time, \n# this scenario corresponds to a Poisson distribution. \n# On average, Amir responds to 4 leads each day. \n\n\n# Import poisson from scipy.stats\nfrom scipy.stats import poisson\n\n# Probability that Amir responds to 5 leads in a day\nprob_5 = poisson.pmf(5, 4)\nprint(prob_5)\n\n# Amir's coworker responds to an average of 5.5 leads per day.\n# Probability of 5 responses from the coworker\nprob_coworker = poisson.pmf(5, 5.5)\nprint(prob_coworker)\n\n# probability that Amir responds to 2 or fewer leads in a day\nprob_2_or_less = poisson.cdf(2,4)\nprint(prob_2_or_less)\n\n# probability that Amir responds to more than 10 leads in a day\nprob_over_10 = 1- poisson.cdf(10,4)\nprint(prob_over_10)\n","metadata":{},"cell_type":"code","id":"f14ca15b-2ac1-45bf-913c-21e91be09739","outputs":[],"execution_count":null},{"source":"### More probability distributions ###\n\n#-------------------------------------------------\n# Modeling time between leads\n\n# To further evaluate Amir's performance, check how much time it takes him \n# to respond to a lead after he opens it. On average, he responds to \n# 1 request every 2.5 hours. \n\n# Import expon from scipy.stats\nfrom scipy.stats import expon\n\n# Print probability response takes < 1 hour\nprint(expon.cdf(1, scale=2.5))\n\n# Print probability response takes > 4 hours\nprint(1 - expon.cdf(4, scale=2.5))\n\n# Print probability response takes 3-4 hours\nprint(expon.cdf(4, scale=2.5) - expon.cdf(3, scale=2.5))","metadata":{},"cell_type":"code","id":"cd93fde7-01ab-4316-bc57-413005035826","outputs":[],"execution_count":null},{"source":"# 4. Correlation and Experimental Design\n\n## Correlation\n- correlation coefficients: \n    - a number represents relationships between two numeric variables.\n    - it is a number between -1 and 1.\n    - the magnitude corresponds to the strength of the relationship\n    - the sign corresponds the direction of the relationship.\n- magnitude: \n    - the coeff is closer to $1$ when data points are closely clustered around a line, very strong relationship.\n    - Smaller than $1$ the coeff is, more spread out data points are.\n    - the coeff is closer ot $0$, no relationship, the scatter plot looks random.\n- sign:\n    - positive coeff: as $x$ increases, $y$ increases.\n    - negavite coeff: as $x$ increases, $y$ decreases.\n- **Visualising relationships**:\n    - Use /seaborn/ which is a plotting package built on top of matplotlib. \n    - import seaborn as sns\n    - sns.scatterplot(x='variable_on_x', y='variable_on_y', data=df)\n    - adding a trendline: sns.lmplot(x, y, data=df, ci=None), ci=confidence interval margin\n- computing correlation: df['col1'].corr(df['col2']\n- In this course, Pearson-moment correlation is used ($r$) \n    - $\\bar{x}$ = mean of $x$\n    - $\\sigma_x$ = standard deviation of $x$\n    - $$r=\\sum \\frac{ (x_i- \\bar{x})(y_i- \\bar{y})}{\\sigma_x \\times \\sigma_y}$$\n- There are different variations on this formula such as Kendall's $\\tau$ and Spearman's $\\rho$\n\n## Correlation caveats   \n- The correlation coefficient measures the strength of only linear relationships.\n- Do not use correlation function in Python blindly, visualize your data first.\n- Non-linear relationships:  \n    - when data has no nonlinear relationships, use transformation.\n    - For highly skewed data, use log transformation, np.log()\n    - Other transformations: \n        - square root transformation (sqrt(x))\n        - reciprocal transformations (1/x)\n        - and combination sof these...\n- Correlation does not apply causation!!! but association!!!\n    - (x is correlated with y) does not mean (x causes y).\n    - Spurious correlation\n    - Confounder or lurking variable\n \n## Design of experiments\n- \"What is the effect of the treatment on the response?\" \n    - treatment: explanatory/ independent variable\n    - response: response/ dependent variable\n- controlled experiments: \n    - treatment group or control group\n        - treatment group sees advertiement\n        - contol group does not see\n    - groups should be comparable to infer the causation\n    - If not comparable, this could lead to confounding (bias)\n- The gold standard of experiments:\n    - randomized controlled trial: \n        - random assignment of participants to groups (control or treatment)\n        - this ensures that groups are comparable\n    - placebo: \n        - resembles treatment, but has no effect\n        - participants has no idea which group they are belong to.\n        - common in clinical trials\n    - double-blind trial:\n        - person administering the treatment or running the experiment also doesn't know whether the treatment is real or placebo.\n        - prevents bias in the response and/or the analysis of the results\n- Observational studies: \n    - participants are not randomly assigned to groups. \n    - Instead, participants assign themselves, usually based on pre-existing characteristics.\n    - Groups are not random.\n    - Establish association, not causation.\n- Longitudinal vs. cross-sectional studies:\n    i) Longitudinal study: \n        - the same participants are followed over a period of time to examine the effect of treatment on the response. \n    ii) Cross-sectional study:\n        - data is collected from a single snapshot in time. \n        - To investigate the effect of age on height, this study would m","metadata":{},"cell_type":"markdown","id":"d1159e39-ac58-4c78-b14d-0c4e1d78e793"},{"source":"### Correlation ###\n#-------------------------------------------------\n# Relationships between variables\n\n# Work with a dataset world_happiness containing results from the 2019 World Happiness Report.\n\n# Create scatterplot of happiness_score vs life_exp with trendline\nsns.lmplot(x='life_exp', y='happiness_score', data=world_happiness, ci=None)\nplt.show()\n\n# Correlation between life_exp and happiness_score\ncor = world_happiness['life_exp'].corr(world_happiness['happiness_score'])\nprint(cor)\n","metadata":{},"cell_type":"code","id":"77aad92e-2032-458b-bf07-cc7ca09f7d0d","outputs":[],"execution_count":null},{"source":"### Correlation caveats  ###\n#-------------------------------------------------\n# What can't correlation measure?\n\n# Explore one of the caveats of the correlation coefficient by examining \n# the relationship between a country's GDP per capita (gdp_per_cap) and happiness score.\n\n# Scatterplot of gdp_per_cap and life_exp\nsns.scatterplot(x='gdp_per_cap', y='life_exp', data=world_happiness)\nplt.show()\n  \n# Correlation between gdp_per_cap and life_exp\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['life_exp'])\nprint(cor)\n\n# NOTE: Correlation only measures linear relationships.\n\n#-------------------------------------------------\n# Transforming variables\n\n# When variables have skewed distributions, they often require a transformation \n# in order to form a linear relationship with another variable so that correlation can be computed.\n\n# i)  \n# Scatterplot of happiness_score vs. gdp_per_cap\nsns.scatterplot(y='happiness_score', x='gdp_per_cap', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n# ii) Apply log transformation\n# Create log_gdp_per_cap column\nworld_happiness['log_gdp_per_cap'] = np.log(world_happiness['gdp_per_cap'])\n\n# Scatterplot of happiness_score vs. log_gdp_per_cap\nsns.scatterplot(x='log_gdp_per_cap', y='happiness_score', data=world_happiness)\nplt.show()\n\n# Calculate correlation\ncor = world_happiness['log_gdp_per_cap'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n#-------------------------------------------------\n# Does sugar improve happiness?\n\n# A new column has been added to world_happiness called grams_sugar_per_day, \n# which contains the average amount of sugar eaten per person per day in each country. \n# Examine the effect of a country's average sugar consumption on its happiness score.\n\n# Scatterplot of grams_sugar_per_day and happiness_score\nsns.scatterplot(x='grams_sugar_per_day',y='happiness_score', data=world_happiness)\nplt.show()\n\n# Correlation between grams_sugar_per_day and happiness_score\ncor = world_happiness['grams_sugar_per_day'].corr(world_happiness['happiness_score'])\nprint(cor)\n\n# Comment: Increased sugar consumption is associated with a higher happiness score.\n","metadata":{},"cell_type":"code","id":"897c4352-7ce2-44ff-9cc0-6002b76352af","outputs":[],"execution_count":null},{"source":"### Design of experiments  ###\n#-------------------------------------------------\n#\n\n#-------------------------------------------------\n#\n\n#-------------------------------------------------\n#","metadata":{},"cell_type":"code","id":"120e0043-3019-41bf-97e8-2bb6e6579c80","outputs":[],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}